{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "workdir = \"datasets/Project/\"        \n",
    "os.chdir(workdir)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(path, seed=9527):\n",
    "    \"\"\"\n",
    "    Usage: there should be ./breast/ and ./prostate/ under the path\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed=seed)\n",
    "    \n",
    "    def image2tensor(tumor, cnv):\n",
    "        train1, train2, train3, testset1, testset2, testset3 = [], [], [], [], [], []\n",
    "        train = [train1, train2, train3] # 3 channels\n",
    "        test = [testset1, testset2, testset3]\n",
    "\n",
    "        rawfnames = !ls ./$tumor/$cnv/*jpg\n",
    "        rawfnames = np.array(np.sort(rawfnames), dtype=object)\n",
    "        \n",
    "        # randomly distribute data to train set and test set in 5:1 ratio\n",
    "        randomIndices = np.random.permutation(rawfnames.shape[0]//3)\n",
    "        for i, rawf in enumerate(rawfnames):\n",
    "            im = cv2.imread(rawf, 0)\n",
    "            channel = i % 3\n",
    "            if (randomIndices[i//3]) % 6 == 0:\n",
    "                test[channel].append(im)\n",
    "            else:\n",
    "                train[channel].append(im)\n",
    "                \n",
    "        return np.array(train).transpose([1,2,3,0]), np.array(test).transpose([1,2,3,0])\n",
    "    \n",
    "    tumors = [\"breast\", \"prostate\"]\n",
    "    cnvs = [\"altered\", \"flat\"]\n",
    "    \n",
    "    trainset,  testset = {\"breast\" : {}, \"prostate\" : {}}, {\"breast\" : {}, \"prostate\" : {}}\n",
    "    data = {\"trainset\" : trainset , \"testset\" : testset}\n",
    "    \n",
    "    for tumor in tumors:\n",
    "        for i, cnv in enumerate(cnvs):\n",
    "            if i == 0:\n",
    "                trainset[tumor][\"pos\"], testset[tumor][\"pos\"] = image2tensor(tumor, cnv)\n",
    "            if i == 1:\n",
    "                trainset[tumor][\"neg\"], testset[tumor][\"neg\"] = image2tensor(tumor, cnv)\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loaddata(workdir)\n",
    "\n",
    "# fig, axes = plt.subplots(2,4, figsize=(12,5))\n",
    "# i, j = 0, 0\n",
    "\n",
    "# for subset, d1 in data.items():\n",
    "#     for tumor, d2 in d1.items():\n",
    "#         for cnv, imgs in d2.items():\n",
    "#             print(\"{} {} {} {}\".format(subset, tumor, cnv, imgs.shape))\n",
    "#             axes[i,j%4].imshow(data[subset][tumor][cnv][0])\n",
    "#             axes[i,j%4].set_title(\"{} {} {}\".format(subset, tumor, cnv))\n",
    "#             axes[i,j%4].axis('off')\n",
    "#             j += 1\n",
    "#     i += 1\n",
    "\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive CNN with Batch Normalization\n",
    "\n",
    "* Not using any data preprocessing or augmentation\n",
    "* Adding BatchNorm after every ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = \"breast\"\n",
    "\n",
    "xtrain_pos, xtrain_neg = data[\"trainset\"][tumor][\"pos\"], data[\"trainset\"][tumor][\"neg\"]\n",
    "xtest_pos, xtest_neg = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "x_train = np.concatenate((xtrain_pos, xtrain_neg), axis=0)\n",
    "y_train = np.concatenate((np.ones(xtrain_pos.shape[0], dtype=int), np.zeros(xtrain_neg.shape[0], dtype=int)))\n",
    "x_test = np.concatenate((xtest_pos, xtest_neg), axis=0)\n",
    "y_test = np.concatenate((np.ones(xtest_pos.shape[0], dtype=int), np.zeros(xtest_neg.shape[0], dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Not using data augmentation.\n",
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 939 samples, validate on 189 samples\n",
      "Epoch 1/3\n",
      "939/939 [==============================] - 168s 179ms/step - loss: 0.7063 - acc: 0.7753 - val_loss: 0.9750 - val_acc: 0.7513\n",
      "Epoch 2/3\n",
      "939/939 [==============================] - 164s 175ms/step - loss: 0.3215 - acc: 0.8839 - val_loss: 0.8019 - val_acc: 0.7407\n",
      "Epoch 3/3\n",
      "939/939 [==============================] - 163s 174ms/step - loss: 0.2374 - acc: 0.9127 - val_loss: 0.8287 - val_acc: 0.7513\n",
      "189/189 [==============================] - 8s 42ms/step\n",
      "Test loss: 0.8286747850438274\n",
      "Test accuracy: 0.7513227532149622\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 3\n",
    "data_augmentation = False\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "# else:\n",
    "#     print('Using real-time data augmentation.')\n",
    "#     # This will do preprocessing and realtime data augmentation:\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "#         rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         # randomly shift images horizontally (fraction of total width)\n",
    "#         width_shift_range=0.1,\n",
    "#         # randomly shift images vertically (fraction of total height)\n",
    "#         height_shift_range=0.1,\n",
    "#         shear_range=0.,  # set range for random shear\n",
    "#         zoom_range=0.,  # set range for random zoom\n",
    "#         channel_shift_range=0.,  # set range for random channel shifts\n",
    "#         # set mode for filling points outside the input boundaries\n",
    "#         fill_mode='nearest',\n",
    "#         cval=0.,  # value used for fill_mode = \"constant\"\n",
    "#         horizontal_flip=True,  # randomly flip images\n",
    "#         vertical_flip=False,  # randomly flip images\n",
    "#         # set rescaling factor (applied before any other transformation)\n",
    "#         rescale=None,\n",
    "#         # set function that will be applied on each input\n",
    "#         preprocessing_function=None,\n",
    "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
    "#         data_format=None,\n",
    "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "#         validation_split=0.0)\n",
    "\n",
    "#     # Compute quantities required for feature-wise normalization\n",
    "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
    "#     datagen.fit(x_train)\n",
    "\n",
    "#     # Fit the model on the batches generated by datagen.flow().\n",
    "#     model.fit_generator(datagen.flow(x_train, y_train,\n",
    "#                                      batch_size=batch_size),\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(x_test, y_test),\n",
    "#                         workers=3)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /Users/zifanzhu/Desktop/DL_project/datasets/Project/saved_models/naive_cnn_with_batchnorm_breast.h5 \n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'naive_cnn_with_batchnorm_breast.h5'\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prostate Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = \"prostate\"\n",
    "\n",
    "xtrain_pos, xtrain_neg = data[\"trainset\"][tumor][\"pos\"], data[\"trainset\"][tumor][\"neg\"]\n",
    "xtest_pos, xtest_neg = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "x_train = np.concatenate((xtrain_pos, xtrain_neg), axis=0)\n",
    "y_train = np.concatenate((np.ones(xtrain_pos.shape[0], dtype=int), np.zeros(xtrain_neg.shape[0], dtype=int)))\n",
    "x_test = np.concatenate((xtest_pos, xtest_neg), axis=0)\n",
    "y_test = np.concatenate((np.ones(xtest_pos.shape[0], dtype=int), np.zeros(xtest_neg.shape[0], dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Not using data augmentation.\n",
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 608 samples, validate on 123 samples\n",
      "Epoch 1/3\n",
      "608/608 [==============================] - 209s 344ms/step - loss: 0.8720 - acc: 0.6628 - val_loss: 0.8856 - val_acc: 0.7073\n",
      "Epoch 2/3\n",
      "608/608 [==============================] - 110s 181ms/step - loss: 0.3437 - acc: 0.8487 - val_loss: 0.6977 - val_acc: 0.7724\n",
      "Epoch 3/3\n",
      "608/608 [==============================] - 96s 158ms/step - loss: 0.1900 - acc: 0.9293 - val_loss: 0.7554 - val_acc: 0.7480\n",
      "123/123 [==============================] - 5s 41ms/step\n",
      "Test loss: 0.7554102537593221\n",
      "Test accuracy: 0.7479674811285686\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 3\n",
    "data_augmentation = False\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "# else:\n",
    "#     print('Using real-time data augmentation.')\n",
    "#     # This will do preprocessing and realtime data augmentation:\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "#         rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         # randomly shift images horizontally (fraction of total width)\n",
    "#         width_shift_range=0.1,\n",
    "#         # randomly shift images vertically (fraction of total height)\n",
    "#         height_shift_range=0.1,\n",
    "#         shear_range=0.,  # set range for random shear\n",
    "#         zoom_range=0.,  # set range for random zoom\n",
    "#         channel_shift_range=0.,  # set range for random channel shifts\n",
    "#         # set mode for filling points outside the input boundaries\n",
    "#         fill_mode='nearest',\n",
    "#         cval=0.,  # value used for fill_mode = \"constant\"\n",
    "#         horizontal_flip=True,  # randomly flip images\n",
    "#         vertical_flip=False,  # randomly flip images\n",
    "#         # set rescaling factor (applied before any other transformation)\n",
    "#         rescale=None,\n",
    "#         # set function that will be applied on each input\n",
    "#         preprocessing_function=None,\n",
    "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
    "#         data_format=None,\n",
    "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "#         validation_split=0.0)\n",
    "\n",
    "#     # Compute quantities required for feature-wise normalization\n",
    "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
    "#     datagen.fit(x_train)\n",
    "\n",
    "#     # Fit the model on the batches generated by datagen.flow().\n",
    "#     model.fit_generator(datagen.flow(x_train, y_train,\n",
    "#                                      batch_size=batch_size),\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(x_test, y_test),\n",
    "#                         workers=3)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /Users/zifanzhu/Desktop/DL_project/datasets/Project/saved_models/naive_cnn_with_batchnorm_prostate.h5 \n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'naive_cnn_with_batchnorm_prostate.h5'\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model transfer: prostate model test on breast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import keras\n",
    "\n",
    "# load trained model\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'naive_cnn_with_batchnorm_prostate.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "del model\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = \"breast\"\n",
    "\n",
    "xtest_pos, xtest_neg = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "x_test = np.concatenate((xtest_pos, xtest_neg), axis=0)\n",
    "y_test = np.concatenate((np.ones(xtest_pos.shape[0], dtype=int), np.zeros(xtest_neg.shape[0], dtype=int)))\n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 9s 46ms/step\n",
      "Test loss: 0.7713835504319932\n",
      "Test accuracy: 0.7089947080486035\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model transfer: breast model test on prostate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import keras\n",
    "\n",
    "# load trained model\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'naive_cnn_with_batchnorm_breast.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "del model\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = \"prostate\"\n",
    "\n",
    "xtest_pos, xtest_neg = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "x_test = np.concatenate((xtest_pos, xtest_neg), axis=0)\n",
    "y_test = np.concatenate((np.ones(xtest_pos.shape[0], dtype=int), np.zeros(xtest_neg.shape[0], dtype=int)))\n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 6s 48ms/step\n",
      "Test loss: 1.0489666206323034\n",
      "Test accuracy: 0.7235772406182638\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "* Both dataset can achieve about 0.75 accuracy.\n",
    "* Model transfer cannot improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mix two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1547, 225, 225, 3)\n",
      "(1547,)\n",
      "(312, 225, 225, 3)\n",
      "(312,)\n"
     ]
    }
   ],
   "source": [
    "tumor = \"breast\"\n",
    "\n",
    "xtrain_pos_b, xtrain_neg_b = data[\"trainset\"][tumor][\"pos\"], data[\"trainset\"][tumor][\"neg\"]\n",
    "xtest_pos_b, xtest_neg_b = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "x_train_b = np.concatenate((xtrain_pos_b, xtrain_neg_b), axis=0)\n",
    "y_train_b = np.concatenate((np.ones(xtrain_pos_b.shape[0], dtype=int), np.zeros(xtrain_neg_b.shape[0], dtype=int)))\n",
    "x_test_b = np.concatenate((xtest_pos_b, xtest_neg_b), axis=0)\n",
    "y_test_b = np.concatenate((np.ones(xtest_pos_b.shape[0], dtype=int), np.zeros(xtest_neg_b.shape[0], dtype=int)))\n",
    "\n",
    "tumor = \"prostate\"\n",
    "\n",
    "xtrain_pos_p, xtrain_neg_p = data[\"trainset\"][tumor][\"pos\"], data[\"trainset\"][tumor][\"neg\"]\n",
    "xtest_pos_p, xtest_neg_p = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "x_train_p = np.concatenate((xtrain_pos_p, xtrain_neg_p), axis=0)\n",
    "y_train_p = np.concatenate((np.ones(xtrain_pos_p.shape[0], dtype=int), np.zeros(xtrain_neg_p.shape[0], dtype=int)))\n",
    "x_test_p = np.concatenate((xtest_pos_p, xtest_neg_p), axis=0)\n",
    "y_test_p = np.concatenate((np.ones(xtest_pos_p.shape[0], dtype=int), np.zeros(xtest_neg_p.shape[0], dtype=int)))\n",
    "\n",
    "x_train = np.concatenate((x_train_b, x_train_p), axis=0)\n",
    "y_train = np.concatenate((y_train_b, y_train_p), axis=0)\n",
    "x_test = np.concatenate((x_test_b, x_test_p), axis=0)\n",
    "y_test = np.concatenate((y_test_b, y_test_p), axis=0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Not using data augmentation.\n",
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1547 samples, validate on 312 samples\n",
      "Epoch 1/10\n",
      "1547/1547 [==============================] - 244s 158ms/step - loss: 0.7222 - acc: 0.7460 - val_loss: 0.9363 - val_acc: 0.7019\n",
      "Epoch 2/10\n",
      "1547/1547 [==============================] - 238s 154ms/step - loss: 0.4088 - acc: 0.8416 - val_loss: 0.7526 - val_acc: 0.7340\n",
      "Epoch 3/10\n",
      "1547/1547 [==============================] - 233s 150ms/step - loss: 0.3065 - acc: 0.8869 - val_loss: 0.7372 - val_acc: 0.7372\n",
      "Epoch 4/10\n",
      "1547/1547 [==============================] - 230s 148ms/step - loss: 0.2072 - acc: 0.9179 - val_loss: 0.7369 - val_acc: 0.7500\n",
      "Epoch 5/10\n",
      "1547/1547 [==============================] - 229s 148ms/step - loss: 0.1738 - acc: 0.9399 - val_loss: 0.8014 - val_acc: 0.7404\n",
      "Epoch 6/10\n",
      "1547/1547 [==============================] - 229s 148ms/step - loss: 0.1291 - acc: 0.9573 - val_loss: 0.7307 - val_acc: 0.7404\n",
      "Epoch 7/10\n",
      "1547/1547 [==============================] - 232s 150ms/step - loss: 0.0902 - acc: 0.9703 - val_loss: 0.7712 - val_acc: 0.7532\n",
      "Epoch 8/10\n",
      "1547/1547 [==============================] - 231s 149ms/step - loss: 0.0725 - acc: 0.9813 - val_loss: 0.7771 - val_acc: 0.7340\n",
      "Epoch 9/10\n",
      "1547/1547 [==============================] - 230s 149ms/step - loss: 0.0633 - acc: 0.9819 - val_loss: 0.8332 - val_acc: 0.7564\n",
      "Epoch 10/10\n",
      "1547/1547 [==============================] - 234s 151ms/step - loss: 0.0509 - acc: 0.9871 - val_loss: 0.8112 - val_acc: 0.7340\n",
      "312/312 [==============================] - 12s 39ms/step\n",
      "Test loss: 0.811183879390741\n",
      "Test accuracy: 0.7339743605026832\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "data_augmentation = False\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "# else:\n",
    "#     print('Using real-time data augmentation.')\n",
    "#     # This will do preprocessing and realtime data augmentation:\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "#         rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         # randomly shift images horizontally (fraction of total width)\n",
    "#         width_shift_range=0.1,\n",
    "#         # randomly shift images vertically (fraction of total height)\n",
    "#         height_shift_range=0.1,\n",
    "#         shear_range=0.,  # set range for random shear\n",
    "#         zoom_range=0.,  # set range for random zoom\n",
    "#         channel_shift_range=0.,  # set range for random channel shifts\n",
    "#         # set mode for filling points outside the input boundaries\n",
    "#         fill_mode='nearest',\n",
    "#         cval=0.,  # value used for fill_mode = \"constant\"\n",
    "#         horizontal_flip=True,  # randomly flip images\n",
    "#         vertical_flip=False,  # randomly flip images\n",
    "#         # set rescaling factor (applied before any other transformation)\n",
    "#         rescale=None,\n",
    "#         # set function that will be applied on each input\n",
    "#         preprocessing_function=None,\n",
    "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
    "#         data_format=None,\n",
    "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "#         validation_split=0.0)\n",
    "\n",
    "#     # Compute quantities required for feature-wise normalization\n",
    "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
    "#     datagen.fit(x_train)\n",
    "\n",
    "#     # Fit the model on the batches generated by datagen.flow().\n",
    "#     model.fit_generator(datagen.flow(x_train, y_train,\n",
    "#                                      batch_size=batch_size),\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(x_test, y_test),\n",
    "#                         workers=3)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Center-crop for breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = \"breast\"\n",
    "\n",
    "xtrain_pos, xtrain_neg = data[\"trainset\"][tumor][\"pos\"], data[\"trainset\"][tumor][\"neg\"]\n",
    "xtest_pos, xtest_neg = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "x_train = np.concatenate((xtrain_pos, xtrain_neg), axis=0)[:,100:130,100:130,:]\n",
    "y_train = np.concatenate((np.ones(xtrain_pos.shape[0], dtype=int), np.zeros(xtrain_neg.shape[0], dtype=int)))\n",
    "x_test = np.concatenate((xtest_pos, xtest_neg), axis=0)[:,100:130,100:130,:]\n",
    "y_test = np.concatenate((np.ones(xtest_pos.shape[0], dtype=int), np.zeros(xtest_neg.shape[0], dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 939 samples, validate on 189 samples\n",
      "Epoch 1/7\n",
      "939/939 [==============================] - 4s 4ms/step - loss: 0.5799 - acc: 0.8094 - val_loss: 0.8028 - val_acc: 0.7725\n",
      "Epoch 2/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.5407 - acc: 0.8328 - val_loss: 0.6687 - val_acc: 0.7619\n",
      "Epoch 3/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4857 - acc: 0.8371 - val_loss: 0.7507 - val_acc: 0.7725\n",
      "Epoch 4/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4991 - acc: 0.8435 - val_loss: 0.7301 - val_acc: 0.7831\n",
      "Epoch 5/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4690 - acc: 0.8562 - val_loss: 0.7127 - val_acc: 0.7725\n",
      "Epoch 6/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4714 - acc: 0.8477 - val_loss: 0.5994 - val_acc: 0.7778\n",
      "Epoch 7/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4709 - acc: 0.8445 - val_loss: 0.7211 - val_acc: 0.7778\n",
      "189/189 [==============================] - 0s 549us/step\n",
      "Test loss: 0.7210682753532652\n",
      "Test accuracy: 0.7777777768316723\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 7\n",
    "data_augmentation = False\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "# else:\n",
    "#     print('Using real-time data augmentation.')\n",
    "#     # This will do preprocessing and realtime data augmentation:\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "#         rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         # randomly shift images horizontally (fraction of total width)\n",
    "#         width_shift_range=0.1,\n",
    "#         # randomly shift images vertically (fraction of total height)\n",
    "#         height_shift_range=0.1,\n",
    "#         shear_range=0.,  # set range for random shear\n",
    "#         zoom_range=0.,  # set range for random zoom\n",
    "#         channel_shift_range=0.,  # set range for random channel shifts\n",
    "#         # set mode for filling points outside the input boundaries\n",
    "#         fill_mode='nearest',\n",
    "#         cval=0.,  # value used for fill_mode = \"constant\"\n",
    "#         horizontal_flip=True,  # randomly flip images\n",
    "#         vertical_flip=False,  # randomly flip images\n",
    "#         # set rescaling factor (applied before any other transformation)\n",
    "#         rescale=None,\n",
    "#         # set function that will be applied on each input\n",
    "#         preprocessing_function=None,\n",
    "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
    "#         data_format=None,\n",
    "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "#         validation_split=0.0)\n",
    "\n",
    "#     # Compute quantities required for feature-wise normalization\n",
    "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
    "#     datagen.fit(x_train)\n",
    "\n",
    "#     # Fit the model on the batches generated by datagen.flow().\n",
    "#     model.fit_generator(datagen.flow(x_train, y_train,\n",
    "#                                      batch_size=batch_size),\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(x_test, y_test),\n",
    "#                         workers=3)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Center-crop for prostate cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = \"breast\"\n",
    "\n",
    "xtrain_pos, xtrain_neg = data[\"trainset\"][tumor][\"pos\"], data[\"trainset\"][tumor][\"neg\"]\n",
    "xtest_pos, xtest_neg = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "x_train = np.concatenate((xtrain_pos, xtrain_neg), axis=0)[:,100:130,100:130,:]\n",
    "y_train = np.concatenate((np.ones(xtrain_pos.shape[0], dtype=int), np.zeros(xtrain_neg.shape[0], dtype=int)))\n",
    "x_test = np.concatenate((xtest_pos, xtest_neg), axis=0)[:,100:130,100:130,:]\n",
    "y_test = np.concatenate((np.ones(xtest_pos.shape[0], dtype=int), np.zeros(xtest_neg.shape[0], dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 939 samples, validate on 189 samples\n",
      "Epoch 1/7\n",
      "939/939 [==============================] - 4s 4ms/step - loss: 0.6661 - acc: 0.7891 - val_loss: 0.9707 - val_acc: 0.7566\n",
      "Epoch 2/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.5117 - acc: 0.8211 - val_loss: 0.7810 - val_acc: 0.7566\n",
      "Epoch 3/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4723 - acc: 0.8509 - val_loss: 0.9053 - val_acc: 0.7566\n",
      "Epoch 4/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4952 - acc: 0.8445 - val_loss: 0.5794 - val_acc: 0.7725\n",
      "Epoch 5/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4857 - acc: 0.8456 - val_loss: 0.6069 - val_acc: 0.7778\n",
      "Epoch 6/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4268 - acc: 0.8616 - val_loss: 0.6018 - val_acc: 0.7725\n",
      "Epoch 7/7\n",
      "939/939 [==============================] - 3s 3ms/step - loss: 0.4466 - acc: 0.8466 - val_loss: 0.6164 - val_acc: 0.7725\n",
      "189/189 [==============================] - 0s 553us/step\n",
      "Test loss: 0.61644564293049\n",
      "Test accuracy: 0.772486771540667\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 7\n",
    "data_augmentation = False\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "# else:\n",
    "#     print('Using real-time data augmentation.')\n",
    "#     # This will do preprocessing and realtime data augmentation:\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "#         rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         # randomly shift images horizontally (fraction of total width)\n",
    "#         width_shift_range=0.1,\n",
    "#         # randomly shift images vertically (fraction of total height)\n",
    "#         height_shift_range=0.1,\n",
    "#         shear_range=0.,  # set range for random shear\n",
    "#         zoom_range=0.,  # set range for random zoom\n",
    "#         channel_shift_range=0.,  # set range for random channel shifts\n",
    "#         # set mode for filling points outside the input boundaries\n",
    "#         fill_mode='nearest',\n",
    "#         cval=0.,  # value used for fill_mode = \"constant\"\n",
    "#         horizontal_flip=True,  # randomly flip images\n",
    "#         vertical_flip=False,  # randomly flip images\n",
    "#         # set rescaling factor (applied before any other transformation)\n",
    "#         rescale=None,\n",
    "#         # set function that will be applied on each input\n",
    "#         preprocessing_function=None,\n",
    "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
    "#         data_format=None,\n",
    "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "#         validation_split=0.0)\n",
    "\n",
    "#     # Compute quantities required for feature-wise normalization\n",
    "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
    "#     datagen.fit(x_train)\n",
    "\n",
    "#     # Fit the model on the batches generated by datagen.flow().\n",
    "#     model.fit_generator(datagen.flow(x_train, y_train,\n",
    "#                                      batch_size=batch_size),\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(x_test, y_test),\n",
    "#                         workers=3)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "workdir = \"datasets/Project/\"        \n",
    "os.chdir(workdir)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(path, seed=9527):\n",
    "    \"\"\"\n",
    "    Usage: there should be ./breast/ and ./prostate/ under the path\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed=seed)\n",
    "    \n",
    "    def image2tensor(tumor, cnv):\n",
    "        train1, train2, train3, testset1, testset2, testset3 = [], [], [], [], [], []\n",
    "        train = [train1, train2, train3] # 3 channels\n",
    "        test = [testset1, testset2, testset3]\n",
    "\n",
    "        rawfnames = !ls ./$tumor/$cnv/*jpg\n",
    "        rawfnames = np.array(np.sort(rawfnames), dtype=object)\n",
    "        \n",
    "        # randomly distribute data to train set and test set in 5:1 ratio\n",
    "        randomIndices = np.random.permutation(rawfnames.shape[0]//3)\n",
    "        for i, rawf in enumerate(rawfnames):\n",
    "            im = cv2.imread(rawf, 0)\n",
    "            channel = i % 3\n",
    "            if (randomIndices[i//3]) % 6 == 0:\n",
    "                test[channel].append(im)\n",
    "            else:\n",
    "                train[channel].append(im)\n",
    "                \n",
    "        return np.array(train).transpose([1,2,3,0]), np.array(test).transpose([1,2,3,0])\n",
    "    \n",
    "    tumors = [\"breast\", \"prostate\"]\n",
    "    cnvs = [\"altered\", \"flat\"]\n",
    "    \n",
    "    trainset,  testset = {\"breast\" : {}, \"prostate\" : {}}, {\"breast\" : {}, \"prostate\" : {}}\n",
    "    data = {\"trainset\" : trainset , \"testset\" : testset}\n",
    "    \n",
    "    for tumor in tumors:\n",
    "        for i, cnv in enumerate(cnvs):\n",
    "            if i == 0:\n",
    "                trainset[tumor][\"pos\"], testset[tumor][\"pos\"] = image2tensor(tumor, cnv)\n",
    "            if i == 1:\n",
    "                trainset[tumor][\"neg\"], testset[tumor][\"neg\"] = image2tensor(tumor, cnv)\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loaddata(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(807, 30, 30, 3)\n",
      "(740, 30, 30, 3)\n",
      "(162, 30, 30, 3)\n",
      "(150, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "tumor = \"breast\"\n",
    "\n",
    "xtrain_pos_b, xtrain_neg_b = data[\"trainset\"][tumor][\"pos\"], data[\"trainset\"][tumor][\"neg\"]\n",
    "xtest_pos_b, xtest_neg_b = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "tumor = \"prostate\"\n",
    "\n",
    "xtrain_pos_p, xtrain_neg_p = data[\"trainset\"][tumor][\"pos\"], data[\"trainset\"][tumor][\"neg\"]\n",
    "xtest_pos_p, xtest_neg_p = data[\"testset\"][tumor][\"pos\"], data[\"testset\"][tumor][\"neg\"]\n",
    "\n",
    "# crop the center\n",
    "\n",
    "x_train_pos = np.concatenate((xtrain_pos_b, xtrain_pos_p), axis=0)[:,100:130,100:130,:]\n",
    "x_train_neg = np.concatenate((xtrain_neg_b, xtrain_neg_p), axis=0)[:,100:130,100:130,:]\n",
    "x_test_pos = np.concatenate((xtest_pos_b, xtest_pos_p), axis=0)[:,100:130,100:130,:]\n",
    "x_test_neg = np.concatenate((xtest_neg_b, xtest_neg_p), axis=0)[:,100:130,100:130,:]\n",
    "\n",
    "\n",
    "print(x_train_pos.shape)\n",
    "print(x_train_neg.shape)\n",
    "print(x_test_pos.shape)\n",
    "print(x_test_neg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VAE for cnv_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zifanzhu/miniconda3/envs/project_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 2700)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1382912     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,384,964\n",
      "Trainable params: 1,384,964\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2700)              1385100   \n",
      "=================================================================\n",
      "Total params: 1,386,636\n",
      "Trainable params: 1,386,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 2700)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 1384964   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 2700)              1386636   \n",
      "=================================================================\n",
      "Total params: 2,771,600\n",
      "Trainable params: 2,771,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test = data\n",
    "    # os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    # filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1])\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    # plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "    z_sample = np.array([[2, -4]])\n",
    "    x_decoded = decoder.predict(z_sample)\n",
    "    image = x_decoded[0].reshape(30, 30, 3)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# load dataset\n",
    "x_train = x_train_pos\n",
    "x_test = x_test_pos\n",
    "\n",
    "original_dim = 30 * 30 * 3\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 32\n",
    "latent_dim = 2\n",
    "epochs = 120\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "\n",
    "reconstruction_loss = mse(inputs, outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "trained=True\n",
    "\n",
    "if not trained:\n",
    "    # train the autoencoder\n",
    "    vae.fit(x_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_test, None))\n",
    "    vae.save_weights('saved_models/vae_cnv_pos.h5')\n",
    "else:\n",
    "    vae.load_weights('saved_models/vae_cnv_pos.h5', by_name=False)\n",
    "    \n",
    "models = (encoder, decoder)\n",
    "data = (x_test)\n",
    "plot_results(models,\n",
    "             data,\n",
    "             batch_size=batch_size,\n",
    "             model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "samp = 30\n",
    "\n",
    "vae_pos = np.zeros((samp * samp, 30, 30, 3))\n",
    "grid_x = np.linspace(-2, 2, samp)\n",
    "grid_y = np.linspace(-2, 2, samp)\n",
    "\n",
    "for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            vae_pos[i * samp + j] = x_decoded[0].reshape(30, 30, 3)\n",
    "            \n",
    "print(vae_pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VAE for cnv_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 2700)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          1382912     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,384,964\n",
      "Trainable params: 1,384,964\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2700)              1385100   \n",
      "=================================================================\n",
      "Total params: 1,386,636\n",
      "Trainable params: 1,386,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 2700)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 1384964   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 2700)              1386636   \n",
      "=================================================================\n",
      "Total params: 2,771,600\n",
      "Trainable params: 2,771,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test = data\n",
    "    # os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    # filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1])\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    # plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "    z_sample = np.array([[2, -4]])\n",
    "    x_decoded = decoder.predict(z_sample)\n",
    "    image = x_decoded[0].reshape(30, 30, 3)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# load dataset\n",
    "x_train = x_train_neg\n",
    "x_test = x_test_neg\n",
    "\n",
    "original_dim = 30 * 30 * 3\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 32\n",
    "latent_dim = 2\n",
    "epochs = 80\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "\n",
    "reconstruction_loss = mse(inputs, outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "trained=True\n",
    "\n",
    "if not trained:\n",
    "    # train the autoencoder\n",
    "    vae.fit(x_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_test, None))\n",
    "    vae.save_weights('saved_models/vae_cnv_neg.h5')\n",
    "else:\n",
    "    vae.load_weights('saved_models/vae_cnv_neg.h5', by_name=False)\n",
    "    \n",
    "models = (encoder, decoder)\n",
    "data = (x_test)\n",
    "plot_results(models,\n",
    "             data,\n",
    "             batch_size=batch_size,\n",
    "             model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "samp = 30\n",
    "\n",
    "vae_neg = np.zeros((samp * samp, 30, 30, 3))\n",
    "grid_x = np.linspace(-2, 2, samp)\n",
    "grid_y = np.linspace(-2, 2, samp)\n",
    "\n",
    "for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            vae_neg[i * samp + j] = x_decoded[0].reshape(30, 30, 3)\n",
    "            \n",
    "print(vae_neg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use VAE generated data as augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3347, 30, 30, 3)\n",
      "(3347,)\n",
      "(312, 30, 30, 3)\n",
      "(312,)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate((x_train_pos, vae_pos, x_train_neg, vae_neg), axis=0)\n",
    "x_test = np.concatenate((x_test_pos, x_test_neg), axis=0)\n",
    "\n",
    "y_train = np.concatenate(((np.ones(x_train_pos.shape[0] + vae_pos.shape[0], dtype=int), \n",
    "                           np.zeros(x_train_neg.shape[0] + vae_neg.shape[0], dtype=int))))\n",
    "y_test = np.concatenate((np.ones(x_test_pos.shape[0], dtype=int), np.zeros(x_test_neg.shape[0], dtype=int)))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 3347 samples, validate on 312 samples\n",
      "Epoch 1/25\n",
      "3347/3347 [==============================] - 11s 3ms/step - loss: 0.8681 - acc: 0.6050 - val_loss: 0.7645 - val_acc: 0.7564\n",
      "Epoch 2/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.8034 - acc: 0.6119 - val_loss: 0.6603 - val_acc: 0.7660\n",
      "Epoch 3/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.7888 - acc: 0.6056 - val_loss: 0.7118 - val_acc: 0.7756\n",
      "Epoch 4/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.7643 - acc: 0.6232 - val_loss: 0.7452 - val_acc: 0.7564\n",
      "Epoch 5/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.7653 - acc: 0.6164 - val_loss: 0.8085 - val_acc: 0.7692\n",
      "Epoch 6/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.7348 - acc: 0.6244 - val_loss: 0.6422 - val_acc: 0.7660\n",
      "Epoch 7/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.7415 - acc: 0.6358 - val_loss: 0.7649 - val_acc: 0.7756\n",
      "Epoch 8/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6995 - acc: 0.6430 - val_loss: 0.6310 - val_acc: 0.7821\n",
      "Epoch 9/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6842 - acc: 0.6504 - val_loss: 0.7678 - val_acc: 0.7660\n",
      "Epoch 10/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6765 - acc: 0.6433 - val_loss: 0.6705 - val_acc: 0.7724\n",
      "Epoch 11/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6637 - acc: 0.6576 - val_loss: 0.7110 - val_acc: 0.7628\n",
      "Epoch 12/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6580 - acc: 0.6672 - val_loss: 0.6594 - val_acc: 0.7660\n",
      "Epoch 13/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6389 - acc: 0.6764 - val_loss: 0.6307 - val_acc: 0.7692\n",
      "Epoch 14/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6298 - acc: 0.6988 - val_loss: 0.6891 - val_acc: 0.7596\n",
      "Epoch 15/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6259 - acc: 0.7132 - val_loss: 0.6619 - val_acc: 0.7788\n",
      "Epoch 16/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6007 - acc: 0.6950 - val_loss: 0.7613 - val_acc: 0.7564\n",
      "Epoch 17/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.6033 - acc: 0.7156 - val_loss: 0.7774 - val_acc: 0.7596\n",
      "Epoch 18/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.5753 - acc: 0.7209 - val_loss: 0.6071 - val_acc: 0.7564\n",
      "Epoch 19/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.5635 - acc: 0.7338 - val_loss: 0.7536 - val_acc: 0.7628\n",
      "Epoch 20/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.5342 - acc: 0.7392 - val_loss: 0.6786 - val_acc: 0.7724\n",
      "Epoch 21/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.5107 - acc: 0.7592 - val_loss: 0.6240 - val_acc: 0.7981\n",
      "Epoch 22/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.5249 - acc: 0.7517 - val_loss: 0.6320 - val_acc: 0.7788\n",
      "Epoch 23/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.5009 - acc: 0.7738 - val_loss: 0.6393 - val_acc: 0.7949\n",
      "Epoch 24/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.4857 - acc: 0.7729 - val_loss: 0.6306 - val_acc: 0.8077\n",
      "Epoch 25/25\n",
      "3347/3347 [==============================] - 10s 3ms/step - loss: 0.4864 - acc: 0.7783 - val_loss: 0.7099 - val_acc: 0.7885\n",
      "312/312 [==============================] - 0s 533us/step\n",
      "Test loss: 0.7098580149885936\n",
      "Test accuracy: 0.7884615384615384\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 25\n",
    "data_augmentation = False\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "# else:\n",
    "#     print('Using real-time data augmentation.')\n",
    "#     # This will do preprocessing and realtime data augmentation:\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "#         rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         # randomly shift images horizontally (fraction of total width)\n",
    "#         width_shift_range=0.1,\n",
    "#         # randomly shift images vertically (fraction of total height)\n",
    "#         height_shift_range=0.1,\n",
    "#         shear_range=0.,  # set range for random shear\n",
    "#         zoom_range=0.,  # set range for random zoom\n",
    "#         channel_shift_range=0.,  # set range for random channel shifts\n",
    "#         # set mode for filling points outside the input boundaries\n",
    "#         fill_mode='nearest',\n",
    "#         cval=0.,  # value used for fill_mode = \"constant\"\n",
    "#         horizontal_flip=True,  # randomly flip images\n",
    "#         vertical_flip=False,  # randomly flip images\n",
    "#         # set rescaling factor (applied before any other transformation)\n",
    "#         rescale=None,\n",
    "#         # set function that will be applied on each input\n",
    "#         preprocessing_function=None,\n",
    "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
    "#         data_format=None,\n",
    "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "#         validation_split=0.0)\n",
    "\n",
    "#     # Compute quantities required for feature-wise normalization\n",
    "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
    "#     datagen.fit(x_train)\n",
    "\n",
    "#     # Fit the model on the batches generated by datagen.flow().\n",
    "#     model.fit_generator(datagen.flow(x_train, y_train,\n",
    "#                                      batch_size=batch_size),\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(x_test, y_test),\n",
    "#                         workers=3)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
